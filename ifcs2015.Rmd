---
title: "A compositional approach for merging finite mixture components (CoDaWork 2015)"
author: "Marc Comas-Cufí, Josep Antoni Martín-Fernández and Glòria Mateu-Figueras"
date: "13 May 2015"
output: 
  html_document: 
    toc: yes
---

```{r, include=FALSE}
library(ggplot2)
library(mclust)
library(dplyr)
library(mixpack)
library(grid)
library(gridBase)
options(width=200)
knitr::opts_chunk$set(comment = " ", echo = FALSE, warning = FALSE)
rnormmix = function(n, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1)){
  df = apply(cbind(mean, sd), 1, function(pars) rnorm(n, mean=pars[1], sd=pars[2]))
  z = sapply(sample(1:3, size = n, replace = TRUE, prob = pi), function(i) 1:3 == i)
  df[t(z)]
}
dnormmix = function(x, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1)){
  df = apply(cbind(pi, mean, sd), 1, function(pars) pars[1] * dnorm(x, mean=pars[2], sd=pars[3]))
  apply(df, 1, sum)
}
cnormmix = function(x, pi= c(1/3, 1/3, 1/3), 
                    mean = c(-2, 0, 2),
                    sd = c(1, 1, 1),
                    class = 1:length(pi)){
  df = apply(cbind(pi, mean, sd), 1, function(pars) pars[1] * dnorm(x, mean=pars[2], sd=pars[3]))
  as.factor(class[apply(df, 1, which.max)])
}
get_sample = function(seed = 2){
  set.seed(seed)
  Pi = c(2/9, 4/9, 3/9)
  Mean = c(-2, 3.5, 5)
  Sd = c(0.65,1.2,0.8)
  df = data.frame('x' = rnormmix(n = 100,  pi = Pi, mean = Mean, sd = Sd))
  df$f = dnormmix(df$x, pi = Pi, mean = Mean, sd = Sd)
  df$class = cnormmix(df$x, pi = Pi, mean = Mean, sd = Sd)
  df$class2 = cnormmix(df$x, pi = Pi, mean = Mean, sd = Sd, class=c(1,2,2))
  df$f1 = Pi[1] * dnorm(df$x, mean = Mean[1], sd = Sd[1]) / df$f
  df$f2 = Pi[2] * dnorm(df$x, mean = Mean[2], sd = Sd[2]) / df$f
  df$f3 = Pi[3] * dnorm(df$x, mean = Mean[3], sd = Sd[3]) / df$f
  df.dens = data.frame('x' = seq(-5, 10, 0.2))
  df.dens$f = dnormmix(df.dens$x, pi = Pi, mean = Mean, sd = Sd)
  df.dens$f1 = Pi[1] * dnorm(df.dens$x, mean = Mean[1], sd = Sd[1])
  df.dens$f2 = Pi[2] * dnorm(df.dens$x, mean = Mean[2], sd = Sd[2])
  df.dens$f3 = Pi[3] * dnorm(df.dens$x, mean = Mean[3], sd = Sd[3])
  list('sample' = df, 'density' = df.dens)
}
```

# Introducing the purpose of merging components

Let's assume we have the following sample $X = \{x_1,\dots,x_{100} \}$:

```{r, fig.width=4, fig.height=3}
v_xlim = c(-5, 8)
X = get_sample()
df = X$sample
df.dens = X$density

(p1 <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white') +
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01), alpha=1) +
  theme_bw() + xlim(v_xlim) + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),  panel.grid=element_blank()) +
  ylab(NULL) + xlab(NULL))
```

Just looking to the plot, _how many groups do you see?_ I think, it is natural to see two group; one formed with positive values and another formed with the negative ones. 

```{r, fig.width=4, fig.height=3}
df$km = ifelse(df$x < 0, '1', '2')
ggplot() + 
  geom_histogram(data=df, aes(x=x, fill=km), binwidth=0.5, alpha=0.15, col='black') +
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.5, col=km), alpha=1) +
  theme_bw() + xlim(v_xlim) + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),  panel.grid=element_blank()) +
  ylab(NULL) + xlab(NULL)  + theme(legend.position="none")
```

## Common approach in parametric clustering

The most common approach in model based clustering starts by fitting a finite mixture of distribution, for example Gaussian distributions. One way to select the number of components is using the BIC criteria. Using this criteria, we get that the best way to represent our data is using $3$ components

\[
f(x) = \color{black}{\pi_1 f_1(x;\theta_1)} + \color{black}{\pi_2 f_2(x;\theta_2)} + \color{black}{\pi_3 f_3(x;\theta_3)}.
\]

After calculating the parameters $\pi_1, \pi_2, \pi_3$ and $\theta_1, \theta_2, \theta_3$, we get the following mixture

```{r, fig.width=4, fig.height=3}
v_xlim = c(-5, 8)
(p2 <- p1 + geom_line(data=df.dens, aes(x=x, y=f), size = 1))
```

We can think a finite mixture as $3$ different distributions generating sample independently with $\pi_1$, $\pi_2$, $\pi_3$ being the proportions of sample expected to be generated by component $f_1$, $f_2$, $f_3$ respectively.

\[
f(x) = \color{red}{\pi_1 f_1(x;\theta_1)} + \color{green}{\pi_2 f_2(x;\theta_2)} + \color{blue}{\pi_3 f_3(x;\theta_3)} 
\]

```{r, fig.width=4, fig.height=3}
(p3 <- p1 +
  geom_line(data=df.dens, aes(x=x, y=f), size = 1) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2), size = 1, col = 'green', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f3), size = 1, col = 'blue', alpha=0.6)) + 
  theme(legend.position="none", panel.grid=element_blank())
```

To classifiy, the observations are assigned to the Gaussian distribution, $f_j(x;\theta_j)$, they most likely belong to

```{r, fig.width=4, fig.height=3}
(p1c <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white', alpha=0.6) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2), size = 1, col = 'green', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f3), size = 1, col = 'blue', alpha=0.6) + 
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01, col=class), alpha=1) + 
  theme_bw() +  theme(legend.position="none", panel.grid=element_blank()) +xlim(v_xlim) + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL) + xlab(NULL) +  
   scale_color_manual(values = c("red", "green", "blue")))
```

We have considered three components and we have assigned each observation $x_i$ to the component $j$ such that posterior probability

\[
\tau_{ij} = \frac{\pi_j f_j(x_i;\theta_j)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}
\]

is maximum, in other words, to the highest component

\[
\left(\color{red}{\frac{\pi_1 f_1(x_i;\theta_1)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}}, \color{green}{\frac{\pi_2 f_2(x_i;\theta_2)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}}, \color{blue}{\frac{\pi_3 f_3(x_i;\theta_3)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}} \right)
\]

or simplifying, to the highest component

\[
\left(\color{red}{\pi_1 f_1(x_i;\theta_1)}, \color{green}{\pi_2 f_2(x_i;\theta_2)}, \color{blue}{\pi_3 f_3(x_i;\theta_3)} \right)
\]

As we can see, the number of clusters differs from the number we guess at the beginning.  

A different approach consists in considering a mixture of Gaussian mixtures, and classify according to the corresponding components. For example, to decompose mixture $f$ with the following two components

\[
f(x) = \color{red}{\pi_1 f_1(x;\theta_1)} + \color{green}{ (\pi_2+\pi_3)\left\{ \frac{\pi_2}{\pi_2+\pi_3} f_2(x;\theta_2) + \frac{\pi_3}{\pi_2+\pi_3} f_3(x;\theta_3) \right\}}
\]

and classify observations to one of the two components most likely belong to

\[
\left(\color{red}{\pi_1 f_1(x_i;\theta_1)}, \color{green}{\pi_2 f_2(x_i;\theta_2)}+\color{green}{\pi_3 f_3(x_i;\theta_3)} \right)
\]

```{r, fig.width=4, fig.height=3, fig.show='hold'}
(p32 <- p1 +
  geom_line(data=df.dens, aes(x=x, y=f), size = 1) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2+f3), size = 1, col = 'green', alpha=0.6))

(p12c <- ggplot() + 
  geom_histogram(data=df, aes(x=x, y = ..density..), binwidth=0.5, col='black', fill='white', alpha=0.6) +
  geom_line(data=df.dens, aes(x=x, y=f1), size = 1, col = 'red', alpha=0.6) + 
  geom_line(data=df.dens, aes(x=x, y=f2+f3), size = 1, col = 'green', alpha=0.6) + 
  geom_segment(data=df, aes(x=x, xend=x, y=0, yend=0.01, col=class), alpha=1) + 
  theme_bw() + theme(legend.position="none", panel.grid=element_blank()) +xlim(v_xlim) + 
   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + ylab(NULL) + xlab(NULL) +  
   scale_color_manual(values = c("red", "green", "green")))
```

Following this approach we are modelling our data with the same mixture but considering different components as a single cluster.

## Hierarchical mixture sequence

This lead us to the following strategy, starting from a finite mixture with a certain number of components, we repeatedly merge components "more likely" to be considered as a single cluster

```{r, include=FALSE}
set.seed(3)
ms = c(runif(3, min = -2 , max= -1), runif(3, min = 0 , max= 1), runif(3, min = 2 , max= 3))
x = Reduce('c', lapply(ms, function(m) rnorm(1000, m, 0.1)))
mc = Mclust(x, G = 2:10)

df.d = data.frame(
  x = x,
  class = mc$classification)

hp = get_hierarchical_partition(mc$z, lambda = function(v_tau, a, b) if(which.max(v_tau) == b) 1 else 0, omega = function(v_tau, a) v_tau[a])
```

```{r, include=FALSE}
draw_partition = function(LVL){
  p = ggplot() + 
    geom_histogram(data=df.d, aes(x=x, y = ..density..), binwidth=0.1, col='black', fill='white', alpha=0.6)
  
  for(i in hp[[LVL]]){
    df.d.dens = data.frame(x = seq(-3, 4, length=1000)) 
    func = function(x) Reduce(`+`, lapply(i, function(i) mc$parameters$pro[i] * dnorm(x, mean = mc$parameters$mean[i], sd = sqrt(mc$parameters$variance$sigmasq) )))
    p = p + geom_line(data=df.d.dens %>% mutate(f = func(x)), aes(x=x, y=f), size = 1, col = rainbow(8)[1+(3+i[[1]])%%8], alpha=0.99)
  }
  
  p + theme_bw() +  theme(legend.position="none") + 
    theme(axis.ticks = element_blank(), 
          axis.text = element_blank(),
          panel.grid=element_blank()) + ylab(NULL) + ylim(0,1)
}
```

```{r, include=FALSE}
library(dendextend)

set_diff = lapply(8:2, function(i) unlist(setdiff(hp[[i-1]], hp[[i]])))
cummulative = lapply(1:7, function(till, set_diff){
  if(till>1)
    Reduce('union', set_diff[1:(till-1)])
  else
    integer(0)
}, set_diff)
set_diff2 = mapply(intersect, set_diff, cummulative)

leafs = mapply(setdiff, set_diff, set_diff2)

a = list()
a$merge = do.call('rbind', lapply(1:7, function(i){
  if(length(leafs[[i]]) == 2){
    return(-leafs[[i]])
  }
  if(length(leafs[[i]]) == 1){
    i1 = max(which(sapply(set_diff[1:(i-1)], 
                          function(s) 
                            length(setdiff(setdiff(set_diff[[i]], leafs[[i]]), s)) == 0)))
    return(c(-leafs[[i]], i1))
  }
  if(length(leafs[[i]]) == 0){
    i1 = max(which(sapply(set_diff[1:(i-1)], 
                          function(s) 
                            length(setdiff(s, setdiff(set_diff[[i]], leafs[[i]]))) == 0)))
    i2 = max(which(sapply(set_diff[1:(i-1)], 
                          function(s) 
                            length(setdiff(s, setdiff(setdiff(set_diff[[i]], set_diff[[i1]]), leafs[[i]]))) == 0)))
    return(c(i1, i2))
  }
}))
#a$merge = ifelse(a$merge<0, -9-a$merge, a$merge)
a$height <- 1:nrow(a$merge)
a$order <- hp[[1]][[1]]
a$labels <- rep("", 8)#hp[[1]][[1]]
class(a) <- "hclust"        # make it an hclust object

hc = as.dendrogram(a)

# Function to color branches
colbranches <- function(n, col)
{
  a <- attributes(n) # Find the attributes of current node
  # Color edges with requested color
  attr(n, "edgePar") <- c(a$edgePar, list(col=col, lwd=2))
  n # Don't forget to return the node!
}

plt_den = function(LVL, h){
  d1=color_branches(rev(hc), k=LVL, col = rainbow(8)[1+(3+sapply(hp[[LVL]], function(i) i[[1]]))%% 8])
  d1 = assign_values_to_branches_edgePar(d1, value = 3, edgePar = "lwd")
  
  plot(d1, lwd=20, lty = 3, axes = FALSE)
  rect(0, h, 10, 10, col='white', border=NA)
}
```

<video controls="controls" loop="loop"><source src="merging_animation.webm" />Mergin animation</video>

```{r, fig.width=8.5, fig.height=3, fig.show='animate', eval=F}
#
# To generate the animation merging_animation.webm this chunk needs to be executed inside generation. Removing the eval=F. For me, it only worked with MacOSX.
#
#
# for(i in c(8,8:1,1,1,1)){
#   print(draw_partition(i))
# }
oldpar <- par(mar=c(0,0,0,0))
plot.new()
rect(-100, -100, 100, 100, col='white', border=NA)
text(0.5,0.5,"Merging components hierarchically", cex=2, col='blue')
plot.new()
rect(-100, -100, 100, 100, col='white', border=NA)
text(0.5,0.5,"Merging components hierarchically", cex=2, col='blue')

#vsecond <- viewport(width = 0.15, height = 0.2, x = 0.85, y = 0.75)
draw_partition(8)
for(lvl in c(8:1,1)){
  print(draw_partition(lvl))
  #pushViewport(vsecond)
  #par(new=TRUE, fig=gridFIG(), mar=c(0,0,0,0))
  par(new=TRUE, fig=c(0.75,0.95,0.65,0.85), mar=c(0,0,0,0))
  plt_den(lvl, 8.5-lvl)
  par(oldpar)
}
# draw_partition(8)
# library(animation)
# 
# saveGIF(gen(), movie.name = "/Users/marc/Research/CODAWORK2015/merging.gif", ani.width=850, ani.height=300)
```


# What strategies are used to merge components?

There are different approaches to merge the components of a finite mixture 

* The ridgeline unimodal method [Ray and Lindsay (2005)]
* The ridgeline ratio method [Hennig (2010)]
* The dip test method [Tantrum et al. (2003)]
* The Bhattacharyya distance method [Hennig (2010)]
* Directly estimated misclassification probabilities (DEMP) method [Hennig (2010)]
* The prediction strength method [Tibshirani and Walther (2005)]
* Entropy minimization method [Baudry et el. (2010)]

From all of them, here we are interested on those methods depending only on the posterior probabilities

\[
\tau_{ij} = \frac{\pi_j f_j(x_i;\theta_j)}{\sum_{\ell=1}^3 \pi_\ell f_\ell(x_i;\theta_\ell)}.
\]

That is methods which only use the posterior probability vectors

\[
\tau_{i} = \left( \tau_{i1}, \tau_{i2}, \dots, \tau_{ik} \right)
\]

From the previous list only the DEMP method and the Entropy minimization method are based on the posterior probability vectors. Next we revise this two methods and discuss the notion of confusion between component they are considering.


## The entropy criteria

Baudry _et al._ proposed to combine those two components for which after merging the components the total Shannon Entropy is minimized. The Shannon entropy is

\[
- \sum_{i=1}^n \sum_{j=1}^k \tau_{ij} log(\tau_{ij}).
\]

As suggested by Baudry _et al._, this is equivalent to find those two components $I_a$ and $I_b$ maximizing

\[
\sum_{i=1}^n  (\tau_{i I_a}+\tau_{i I_b}) \log(\tau_{i I_a} + \tau_{i I_b}) - 
\sum_{i=1}^n \left\{ \tau_{i I_a} \log(\tau_{i I_a}) + \tau_{i I_b} \log(\tau_{i I_b})\right\}.
\]

The entropy criteria depends on the other components. In fact, it gets smaller when the sum of the other components increase.

```{r, include=F}
library(ggtern)
```

```{r, fig.width=6.5, fig.height=5}
xlog = function(x) ifelse(x == 0, 0, x * log(x))
entr = function(x) -(xlog(x) + xlog(1-x))
p = ggplot()
for(i  in 1:5){
  other = seq(0.2, 0.8, length.out=5)[i]
  p = p + geom_line(data = data.frame(x = seq(0, 1-other, 0.0001)) %>% mutate(ent_diff = xlog(1-other) - xlog(x) - xlog(1-other-x)), aes(x = x, y = ent_diff), col=i)
}
pentropy = p + theme_classic() + 
  xlab(expression(tau[2])) + ylab('Shannon entropy') + coord_cartesian(xlim=c(0,1)) + theme(panel.grid=element_blank())

pscale = ggtern()
for(i  in 1:5){
  other = seq(0.2, 0.8, length.out=5)[i]
  pscale = pscale + geom_Tline(Tintercept=other, col=i)
}
pscale = pscale +
  Tlab(expression(1-tau[1]-tau[2])) + Llab(expression(tau[1])) + Rlab(expression(tau[2])) + 
  theme_classic() + theme(panel.grid.tern=element_blank(), 
                     axis.tern.ticks=element_blank(), 
                     axis.tern.text.T=element_blank(),
                     axis.tern.text.R=element_blank(),
                     axis.tern.text.L=element_blank(),
                     axis.tern.showarrows = F)

plot.new()
vprincipal <- viewport(width = 1, height = 1, x = 0.5, y = 0.5)
vscale <- viewport(width = 0.32, height = 0.4, x = 0.85, y = 0.8)
print(pentropy, vp = vprincipal)
print(pscale, vp=vscale)
```

### Our proposal

Instead of considering the difference of entropies it seems reasonable to consider the aitchison norm of the subcomposition formed with components $I_a$ and $I_b$. To states the problem in similar optimization settings, we propose to maximize the norm multiplied by $-1$.

```{r, fig.width=6.5, fig.height=5}
p = ggplot()
for(i  in 1:5){
  other = seq(0.2, 0.8, length.out=5)[i]
  p = p + geom_line(data = data.frame(x = seq(0, 1-other, 0.0001)) %>% mutate(ent_diff = -log((1-other-x)/x)^2), aes(x = x, y = ent_diff), col=i)
}
paitchison = p + theme_classic() + 
  xlab(expression(tau[2])) + ylab('Minus the Aitchison norm') + coord_cartesian(xlim=c(0,1), ylim=c(-5, 1)) + theme(panel.grid=element_blank())

plot.new()
vprincipal <- viewport(width = 1, height = 1, x = 0.5, y = 0.5)
vscale <- viewport(width = 0.32, height = 0.4, x = 0.85, y = 0.8)
print(paitchison, vp = vprincipal)
print(pscale, vp=vscale)
```

```{r, include=F}
detach("package:ggtern", unload=TRUE)
```


## The missclassification criteria

Hennig proposed to merge those two components $I_a$ and $I_b$ for which the probability of assigning one observation generated by component $I_a$ to $I_b$ is maximum. To approximate the probability 

\[
P(\{x_i \text{ assigned to } I_b\}|\{x_i \text{ generated by } I_a\}) = 
\frac{\color{blue}{P( \{x_i \text{ assigned to } I_b\}, \{x_i \text{ generated by } I_a\})} }{ \color{green}{P(\{x_i \text{ generated by } I_a\})} }
\]

he proposed the following estimator

\[
\frac{
\color{blue}{\frac{1}{n} \sum_{i=1}^n {\tau_{i I_a} \mathbb{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{i I_j} \right)}}
}{ 
\color{green}{\pi_{I_a}}.
}
\]

The estimator can be written down as 

\[
\frac{ \sum_{i=1}^n {\tau_{i I_a} \mathbb{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{i I_j} \right)}}{\sum_{i=1}^n \tau_{i I_a} }
\]


Conditioning that $x_i$ was generated by component 1, the higher the proportion of $\tau_2$ with respect $\tau_1$ higher the confusion between component 1 and 2. To measure this confusion Hennig proposes to count the number of times an observation is classified to component $I_b$ weighted by $\tau_{i I_a}$.

Suppose we have three components. In the following figure the notion of confusion for an observation is given for different values of $\tau_{i1}$, $\tau_{i2}$ and $\tau_3$.

```{r, fig.width=3.5, fig.height=4}
d <- lapply(steps <- c(0, 0.25, 0.5), function(other) data.frame(x = seq(0, 1-other, 0.001)) %>% 
                                                            mutate(demp = ifelse(1-other-x < x & other < x, 1, 0),
                                                                   other = as.character(other))) %>% bind_rows %>% mutate(
                                                                     other = factor(other, labels = sprintf("tau[3]==%3.1f",steps)))

ggplot() + geom_point(data = d, aes(x = x, y = demp, col=other), shape=15, size=1) + theme_bw() + 
  xlab(expression(tau[2])) + ylab('DEMP criteria') + coord_cartesian(xlim=c(0,1)) + facet_grid(other~., labeller = label_parsed) + 
  theme(panel.grid=element_blank(),
        strip.text.y = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="black", fill="white")) + scale_colour_discrete(guide=FALSE)
```

In this particular case (when we have three components) it is easily to see that when $\tau_{i 3}$ is higher than 0.5, the confusion measure is always $0$ no matter the value of $\tau_1$ or $\tau_2$.

### Our proposal

Instead of measuring the confusion of assigning an observation to component $I_b$ given that was generated by $I_a$ with the DEMP approach, we propose to measure the logratio between $\tau_{iI_b}$ and $\tau_{iI_a}$. That is, to merge those two components $I_a$ and $I_b$ maximizing

\[
\frac{ \sum_{i=1}^n {\tau_{i I_a} log(\frac{\tau_{i I_b}}{\tau_{i I_a}})}}{\sum_{i=1}^n \tau_{i I_a} }
\]

```{r, fig.width=3.5, fig.height=4}
d <- lapply(steps <- c(0.0, 0.25, 0.5), function(other) data.frame(x = seq(0, 1-other, 0.001)) %>% 
                                                            mutate(log = log(x/(1-other-x)),
                                                                   other = as.character(other))) %>% bind_rows %>% mutate(
                                                                     other = factor(other, labels = sprintf("tau[3]==%3.1f",steps)))

ggplot() + geom_line(data = d, aes(x = x, y = log, col=other), size=1) + theme_bw() + 
  xlab(expression(tau[2])) + ylab('Log-ratio criteria') + coord_cartesian(xlim=c(0,1)) + facet_grid(other~.,labeller = label_parsed) + 
  theme(panel.grid=element_blank(),
        strip.text.y = element_text(size=12, face="bold"),
        strip.background = element_rect(colour="black", fill="white")) + scale_colour_discrete(guide=FALSE)
```

It is worth noting that in the special case that $\sum_{i=1}^n \tau_{i I_j} = 1$ for each component $I_j$ (defines a probability distribution) the measure is equivalent to the well-known Kullback-Leibler divergence.

# The advantages of a CoDa approach

## Scale invariance

The scale invariance allows us to extend our approach to other measures different that probabilities $\tau_{i}$. For example, our approach can be used in other settings where instead of having the probability we have other relative measurements.


## Subcompositional coherence

The subcompositional coherence allows to consider the following approach. Suppose we know that some subcompositions are going to form a single cluster then, we can find a hierchical merging sequence inside each subcomposition in parallel without considering the other subcompositions.

For example in the following figure, if we knew that the components associated to the leaves of the following tree we going to form a single cluster, the we can calculate the merging inside each subtree.

```{r, fig.width=8.5, fig.height=3}
source("online_code.R")

set.seed(50)
#rnorm(1000, mean=1, sd=100)
x = lapply(1:20, function(m) rnorm(100, mean = m, sd = 200)) %>% unlist
hc = hclust(log(stats::dist(x, method="euclidean")), method="ward.D")

A2Rplot(hc, k = 8, boxes = FALSE, col.up = "gray50", main = 'Subcompositional coherence', show.labels = F)
```

This approach reduced the need to compore all the components. We only need to compare those components in the same cluster in such way to obtain a complete hierachical clustering sequence.

```{r, fig.width=3, fig.height=3}
g = cutree(hc, k=8)

tab = table(g)
g = rep(names(tab), tab)

df = expand.grid(1:2000, 1:2000)

g1 = g[df$Var1]
g2 = g[df$Var2]

df$color = ifelse( g1 == g2, g1, 0)

df = df %>% subset(color != '0')

ggplot() + geom_point(data=df, aes(x = Var1, y=Var2, col=color, fill=color), size=0.5, shape=15) + 
  theme_bw() + theme(legend.position="none", panel.grid=element_blank()) + xlab(NULL) + ylab(NULL)
```

<!--
# A generic formulation

We have seen different approaches to combine components

\[
\frac{\sum_{i=1}^n \omega(\tau_{i \mathcal{P}_s}, I_a) \lambda(\tau_{i \mathcal{P}_s}, I_a, I_b)}{\sum_{i=1}^n \omega(\tau_{i \mathcal{P}_s}, I_a) }
\]
-->


```{r, eval=FALSE}
library(ggtern)
library(compositions)

ggtern() + geom_Tline(
  data=data.frame(levels = c(0.1, 0.2, 0.3, 0.4, 0.5), 
                  other = as.factor(c(0.1, 0.2, 0.3, 0.4, 0.5))), 
  aes(Tintercept=levels, col=other), size=1.5) +
  Tlab(expression(1-tau[1]-tau[2])) + Llab(expression(tau[1])) + Rlab(expression(tau[2])) + 
  theme_classic() + theme(panel.grid.tern=element_blank(), 
                     axis.tern.ticks=element_blank(), 
                     axis.tern.text.T=element_blank(),
                     axis.tern.text.R=element_blank(),
                     axis.tern.text.L=element_blank(),
                     axis.tern.showarrows = F)

plot(df %>% select(f1, f2, f3) %>% acomp, col=df$class)
plot(scale(df %>% select(f1,f2,f3) %>% acomp) %>% data.frame %>% acomp, col=df$class)
# ggtern( data = df %>% select(f1, f2, f3, class) ) + geom_point(aes(x=f1, y=f2, z=f3, col=class)) + theme_bw() + scale_colour_discrete(guide=FALSE)
# 
# df.cent = scale(df %>% select(f1,f2,f3) %>% acomp) %>% data.frame
# df.cent$class = df$class
# ggtern( data = df.cent %>% select(f1, f2, f3, class) ) + geom_point(aes(x=f1, y=f2, z=f3, col=class)) + theme_bw() + scale_colour_discrete(guide=FALSE)
```

# Extra material

* You can get the presentation at [[pdf]](codawork2015.pdf),
* and all the sources at [GIT repository](https://github.com/mcomas/CODAWORK2015)
