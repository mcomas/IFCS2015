---
title: An integrated formulation for merging mixture components based on posterior
  probabilities (IFCS'15)
author: "Marc Comas-Cufí, Josep Antoni Martín-Fernández and Glòria Mateu-Figueras"
date: "July 8th, 2015"
output:
  html_document:
    theme: cosmo
    toc: yes
  pdf_document:
    toc: yes
---

```{r, include=FALSE}
library(ggplot2)
library(mclust)
library(dplyr)
library(mixpack)
library(grid)
library(gridBase)
library(abind)
library(pander)
library(MixSim)
library(Rmixmod)
options(width=200)
knitr::opts_chunk$set(comment = " ", echo = FALSE, warning = FALSE)

source('testing_samples.R')
X = test1(100)
```


# Introduction to the problem of merging components

In this talk we focus on methods used to merge the components of a finite mixture model hierarchically. The merging process is going to be done sequentially. After each merging step, the merged components are considered as a unique component. 

<video controls="controls" loop="loop"><source src="merging_animation.webm" />[[Merging animation]]</video>

No that after merging different components into a single component, we can consider this single component to define a single cluster, and therefore, to group an observation to this cluster if the probability to belong to this single component is highest than the probability to belong to the other components. This approach is classical and it is called the maximum a posterior criterion (MAP criterion).

# Merging components based on the posterior probabilities

We focus only on merging the components of a finite mixture using the information contained only in the posterior probabilities. The problem of interest can be stated as:

> Given a sample of probabilities to belong to each component $I_j$,
>
> \[ \left[ \begin{array}{ccccc}
> (\tau_{11}, & \dots & \tau_{1j}, & \dots & \tau_{1k}), \\
> \vdots      & &    \vdots                     & &    \vdots                     \\
> (\tau_{i1}, & \dots & \tau_{ij}, & \dots & \tau_{ik}), \\
> \vdots      & &      \vdots                   & &       \vdots                  \\
> (\tau_{n1}, & \dots & \tau_{nj}, & \dots & \tau_{nk})
> \end{array} \right] 
> \in \mathcal{S}^k \]
>
> merge the components (sequentially) to obtain a hierarchy over the set of components.

# Our proposal: a generic merging score

To solve the stated problem, we propose:

> Given the set of components $\mathcal{I}_s = \{I_1, \dots, I_s\}$ and $\mathcal{T}_{\mathcal{I}_s} = \{\tau_{1 \mathcal{I}_s}, \dots, \tau_{n \mathcal{I}_s} \}$, we propose to merge those two components $I_a$ and $I_b$, into one component $I_a \cup I_b$ which maximise
\[
S_{\omega, \lambda}( \mathcal{T}_{\mathcal{I}_s}, I_a, I_b) = \frac{\sum_{i=1}^n {\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} {\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}}}{\sum_{i=1}^n {\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)} }},
\]
> 
> where 
> 
>   * the function ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$ measures how likely is to merge component $I_a$ and $I_b$ given the information contained in $\tau_{i \mathcal{I}_s}$ and
>   * the function ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$ measures how relevant is observation $i$ to compute the overall score to merge $I_a$ and $I_b$ given the information contained in $\tau_{i \mathcal{I}_s}$.

Our proposal is generic in the sense that we do not define the measure used to combine the components. The user can define the functions ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$ and ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$ according to their problem.

In next sections, we show different ideas appeared in different works into the presented framework.

## The DEMP approach [Hennig (2010)]

Hennig (2010) proposed a method based on maximizing the Directly Estimated Missclassification Probabilities (DEMP approach). It is easy to show that the DEMP apprach is equivalent to the general score $S_{\omega, \lambda}( \mathcal{T}_{\mathcal{I}_s}, I_a, I_b)$ using 

  * ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = \tau_{i I_{a}}$ and ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = 1{\hskip -2.4 pt}\hbox{I} \left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)$.

## The entropy approach [Baudry et el. (2010)]

Baudry _et al._ (2010) proposed to merged the component of a finite mixture based on minimizing the total Entropy

\[
\sum_{i=1}^n \sum_{j=1}^s \tau_{i \mathcal{I}_j} log(\tau_{i \mathcal{I}_j}).
\]

In this case, it can be seen that their approach is equivalent to the general score $S_{\omega, \lambda}( \mathcal{T}_{\mathcal{I}_s}, I_a, I_b)$ using

  * ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = 1$ and ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = (\tau_{i I_a}+\tau_{i I_b}) \log(\tau_{i I_a} + \tau_{i I_b}) - \left\{ \tau_{i I_a} \log(\tau_{i I_a}) + \tau_{i I_b} \log(\tau_{i I_b})\right\}$

## Longford & Bartosova [Longford & Bartosova (2014)]

Longford & Bartosova (2014) proposed to merge those two components maximizing the probability of classifying an observation to component $I_a$ given that the observation is generated by either $I_a$ or $I_b$. To aproximate such probability a simulated sample $X^*$ of finite mixture with components $\mathcal{I}_s = \{I_1, \dots, I_s\}$. Using the posterior probabilities $\mathcal{T}_{\mathcal{I}_s}^*$ of this simulated sample their approach is equivalent to the score $S_{\omega, \lambda}( \mathcal{T}_{\mathcal{I}_s}, I_a, I_b)$ using

  * ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = 1$ and ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = \frac{\tau_{i I_{b}}}{\tau_{i I_{a}} + \tau_{i I_{b}}}$

# New aproaches using the generic merging score

Each previous methods have considerd different functions ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$; 

* $1{\hskip -2.4 pt}\hbox{I} \left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)$ for the DEMP approach
* $(\tau_{i I_a}+\tau_{i I_b}) \log(\tau_{i I_a} + \tau_{i I_b}) - \left\{ \tau_{i I_a} \log(\tau_{i I_a}) + \tau_{i I_b} \log(\tau_{i I_b})\right\}$ for the entropy approach, and
* $\frac{\tau_{i I_{b}}}{\tau_{i I_{a}} + \tau_{i I_{b}}}$ for the version of Longford & Bartosova

we use the labels `demp`, `entr` and `demp.mod` to refenciate each function respectively. Moreover, for the previous three methods we have seen two different possible definitions of ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$

* $\tau_{i I_{a}}$ for the DEMP approach and
* $1$ for the approach presented by Baudry _et al._ and for the one by Longford & Bartosova.

we refer to them as `prop` and `cnst`. Doing so, we are be able to refer to each method using the label for function ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$ followed by the label refering to function ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}}$. For example, 

* we refer to the DEMP approach as `prop-demp`,
* we refer to Baudry approach as `cnst-entr` and
* we refer to Longford and Bartosova as `cnst-demp.mod`.

Using this labeling, we can define three new approaches corresponding to the labelings `prop-entr`, `prop-demp.mod` and `cnst-demp`.

```{r, results='asis'}
(df <- data.frame(
  'Lambda.Omega' = c('demp', 'demp.mod', 'entr'),
  'cnst' = c('cnst-demp', 'Longford & Bartosova (2014)', 'Baudry et al. (2010)'),
  'prop' = c('Hennig (2010)', 'prop-demp.mod', 'prop-entr')  )) %>% pandoc.table(., style='rmarkdown', split.tables = Inf, emphasize.strong.cols= 1)
```

## Using the indicator variable as a weighting function

In previous approaches we have used either ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = 1$ (`cnst`) or ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = \tau_{i I_{b}}$ (`prop`). In the first each observation is ponderated equally, in the second approach each observation is ponderated according to the posterior probability to belong to $I_a$. A more extrem weighting can be introduced considering only those observations having the posterior of being classified to $I_a$ maximum, that is ${\color{blue}{\omega(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = 1{\hskip -2.4 pt}\hbox{I} \left( \forall j\; \tau_{i I_{a}} \geq \tau_{iI_j} \right)$. We refer to this weighing by `dich`. This new weighting allow us to introduce three new approaches.

```{r, results='asis'}
(df <- cbind(df ,
  data.frame('dich' = c('dich-demp', 'dich-demp.mod', 'dich-entr')))) %>% pandoc.table(., style='rmarkdown', split.tables = Inf, emphasize.strong.cols= 1)
```

## Using the posterior probability of being classified to $I_b$

A very simple function measuring how likely is to classify observation into $I_b$ when the observations are from $I_a$, is to consider simply the posterior probability of classifying to $I_b$, that is ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = \tau_{i I_{b}}$. The measure is similar to the one introduced by Longford & Bartosova, but in this case we are not assuming that the observation has been generated either by component $I_a$ or $I_b$.

We refer to this approach as `prop` (not to be confused with the `prop` labeling used for the function ${\color{blue}{\omega}}$). With this new function we have three new approaches to merge components.

```{r, results='asis'}
(df <- rbind(df, data.frame('Lambda.Omega'='prop', 'cnst' = 'cnst-prop', 'prop' = 'prop-prop', 'dich' = 'dich-prop')) )%>% pandoc.table(., style='rmarkdown', split.tables = Inf, emphasize.strong.cols= 1)
```

## The log-ratio criterias

The log-ratio between the posterior probability of classifying an observation to $I_b$ and the posterior probability of classifying an observation to $I_a$ is a measure of how likely is to classify an observation to $I_b$ when the observation was in fact generated by component $I_a$. This measure can be used to measure how likely is to merge components $I_a$ and $I_b$.

  * Consider ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = log(\tau_{i I_{b}} / \tau_{i I_{a}})$

This approach allow us to introduce three new methods for merging components

```{r, results='asis'}
(df <- rbind(df, data.frame('Lambda.Omega'='log', 'cnst' = 'cnst-log', 'prop' = 'prop-log', 'dich' = 'dich-log')) )%>% pandoc.table(., style='rmarkdown', split.tables = Inf, emphasize.strong.cols= 1)
```

## The Aitchison distance criteria

Finally, using the norm defined on the Simplex (Aitchison) we can have a measure of entropy similar to the one introduced by Baudry et al. (2010). In this case, the measure has some desirable properties like scale-invariance and compositional-coherence.

  * Consider ${\color{red}{\lambda(\tau_{i \mathcal{I}_s}, I_a, I_b)}} = - log(\tau_{i I_{b}} / \tau_{i I_{a}})^2$

We have three new methods

```{r, results='asis'}
(df <- rbind(df, data.frame('Lambda.Omega'='norm', 'cnst' = 'cnst-norm', 'prop' = 'prop-norm', 'dich' = 'dich-norm')) )%>% pandoc.table(., style='rmarkdown', split.tables = Inf, emphasize.strong.cols= 1)
```

# Experiment: comparing the results for each aproach

In this part we would like to see if the previous approaches are usefull to approximate components. We would perform a very simple example, we are going to follow the following steps.
 
 1. We generate a sample from a finite mixture a certain number of components. For example, in the next plot we show three different mixtures with $K_0=3$ components generated using the package `MixSim` using different levels of overlapping $\hat{\omega} \in (0.1, 0.3, 0.5)$ (see Melnykov _et al._ (2012) for details). Note that for each observation we know the component from where it was generated. In other words, we have an initial classification $L_0$ which allow us to compare other clusters with the one given by the components from the generated finite mixture.
 
```{r, fig.width=7, fig.height=2.5}
set.seed(155)
l_df = lapply(c(0.1, 0.3, 0.5), function(omega){
  ms = MixSim(MaxOmega=omega, K=3, p=2, PiLow=0.01, sph=TRUE)
  sim = simdataset(n=1000, Pi=ms$Pi, Mu=ms$Mu, S=ms$S)
  cbind(sim$X,sim$id) %>% data.frame %>% mutate(omega = omega)
}) 

ggplot(data=l_df %>% bind_rows, aes(x=X1, y=X2, col=as.factor(X3))) + geom_point(alpha=0.2) + geom_density2d() + 
  facet_wrap(~omega, scales = 'free')
```

 2. To the data generated in step 1, we fit a mixture more components than the original one. For example, in the previous examples we can fit a finite mixture with 9 components.

```{r, fig.width=7, fig.height=2.5}
set.seed(155)
l_df_7 = lapply(l_df, function(df){
  mod = mixmodCluster(data.frame(df[,c('X1', 'X2')]),
                      nbCluster=7,
                      models=mixmodGaussianModel(listModels="Gaussian_pk_Lk_I"))
  bind_cols(df, data.frame('part' = mod@bestResult@partition), mod@bestResult@proba %>% data.frame %>% setNames(paste0('t', 1:7)))
})

ggplot(data=l_df_7 %>% bind_rows, aes(x=X1, y=X2, col=as.factor(part))) + geom_point(alpha=0.2) + geom_density2d() + 
  facet_wrap(~omega, scales = 'free')
```

After the fitting, each observation has a probability to belong to one component component

```{r}
l_df_7 %>% bind_rows %>% dplyr::select(starts_with('t')) %>% round(5) 
```

  3. With the matrix of posterior probabilities, we build a hierachy using methods explained in this talk. For example, using the DEMP aproach for the first dataset we get the following hierarchy

```{r, fig.width=3.5, fig.height=3.5, results='hold'}
data = l_df_7[[1]] %>% dplyr::select(starts_with('X'))
tau = l_df_7[[1]] %>% dplyr::select(starts_with('t'))
HP = get_hierarchical_partition(tau, 
                                omega = function(v_tau, a) v_tau[a],
                                function(v_tau, a, b) if(which.max(v_tau) == b) 1 else 0)
for(lvl in length(HP):2){
  hp = HP[[lvl]]
  d = data %>% mutate(
    'hp' = cluster_partition(tau = tau, partition = hp),
    'lvl' = lvl)
  p = ggplot(data=d, aes(x=X1, y=X2, col=hp)) + geom_point(alpha=0.2) + geom_density2d() + 
    ggtitle(sprintf('Hierarchy at level %d', lvl)) +
              theme(legend.position="bottom")
  print(p)
}
```

  4. After calculating the hierachy, we compare the components of the initial mixture and the components given at level 3. To compare this components, we compare the clusterings they define using the MAP criterion. For example, in previous example we would like to compare the following two clusterings.
  
```{r, fig.width=3.5, fig.height=3.5, results='hold'}
ggplot(data=l_df[[1]], aes(x=X1, y=X2, col=as.factor(X3))) + geom_point(alpha=0.6) + geom_density2d() +
  ggtitle('Initial MAP partition') +
            theme(legend.position="bottom")

hp = HP[[3]]
d = data %>% mutate(
  'hp' = cluster_partition(tau = tau, partition = hp),
  'lvl' = lvl)
p = ggplot(data=d, aes(x=X1, y=X2, col=hp)) + geom_point(alpha=0.6) + geom_density2d() + 
  ggtitle(sprintf('Hierarchy at level %d', 3)) +
            theme(legend.position="bottom")
print(p)
```

To compare the clustering obtained using the initial MAP partition and the clustering using the three components at level $3$, we compute a measure of agreement between partitions as the classical Adjusted Rand Index (Rand, 1971).

In general, we have considered the following steps. 

  * For differents values of $\hat{\omega} \in (0, 0.5)$,
  
    1. Generate a $X$ sample of $N_{\text{data}}$ observations following a mixtures of $K_0$ components. For each element from sample $X$ keep the label from the component it was generated to obtain a labeling $L_0$. The mixtures components are separated with fixed $\hat{\omega}$ maxoverlapping measure.
    2. Fit a mixture with $K_f$ components ($K_f > K_0$) to sample $X$ and calculate the posterior probabilities $\tau_i = \left(\tau_{i1}, \dots, \tau_{i K_f}\right)$ for each element $x_i \in X$ to obtain $T = \{ \tau_1, \dots, \tau_{N_{\text{data}}} \}$.
    3. Obtain a hierachy $\mathcal{H}$ over the $K_f$ components using a fixed function ${\color{red}{\lambda}}$ and ${\color{blue}{\omega}}$.
    4. Compare the classification given at level $K_0$ of hierarchy $\mathcal{H}$ with the classification given by $L_0$ using the Rand Index to obtain an score $s_i$.

For each function ${\color{red}{\lambda}}$ and ${\color{blue}{\omega}}$ we have repeated the experiment $N_\text{sim}$ times and computes the mean $\bar{s} = \sum_{i=1}^{N_\text{sim}} s_i$

## Results 

A part from the methods presented in this talk, to compare, we have also computed a random hierarchy by randomly merging two components at each step. The results for the random hierarchy have been plot in each plot-cell in blue and compared to each method presented here. We have considered different scenarios showing similar results for this experiment.

* $N_{\text{sim}} = 50$, $N_{\text{data}} = 200$, $K_0=3$, $K_f=9$, $\text{seed}=1$ and $\text{dimension}=5$.

```{r, fig.width=9, fig.height=5}
load('sim01/data/res_NSIM_050-NDATA_200-K0_003-Kf_009-DIM_005-SEED_001.RData')
res.mean = res %>% group_by(omega, lambda, overlap, method) %>% summarise(AR = mean(AR))
ggplot() + geom_line(data = res.mean, aes(x = overlap, y = AR, col = method)) + 
  facet_grid(omega~lambda) + theme_bw()
```

* $N_{\text{sim}} = 50$, $N_{\text{data}} = 200$, $K_0=3$, $K_f=12$, $\text{seed}=1$ and $\text{dimension}=5$.

```{r, fig.width=9, fig.height=5}
load('sim01/data/res_NSIM_050-NDATA_200-K0_003-Kf_012-DIM_005-SEED_001.RData')
res.mean = res %>% group_by(omega, lambda, overlap, method) %>% summarise(AR = mean(AR))
ggplot() + geom_line(data = res.mean, aes(x = overlap, y = AR, col = method)) + 
  facet_grid(omega~lambda) + theme_bw()
```

* $N_{\text{sim}} = 50$, $N_{\text{data}} = 200$, $K_0=3$, $K_f=9$, $\text{seed}=1$ and $\text{dimension}=10$.

```{r, fig.width=9, fig.height=5}
load('sim01/data/res_NSIM_050-NDATA_200-K0_003-Kf_009-DIM_010-SEED_001.RData')
res.mean = res %>% group_by(omega, lambda, overlap, method) %>% summarise(AR = mean(AR))
ggplot() + geom_line(data = res.mean, aes(x = overlap, y = AR, col = method)) + 
  facet_grid(omega~lambda) + theme_bw()
```

# More material

* The presentation [pdf](ifcs2015-pres.pdf)
* The sources for the presentation and the scripts are available at [GitHub](http://github.com/mcomas/IFCS2015)
